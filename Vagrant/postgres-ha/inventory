# This is example inventory file!
# Please specify the ip addresses and connection settings for your environment
# The specified ip addresses will be used to listen by the cluster components.

# "postgresql_exists='true'" if PostgreSQL is already exists and running
# "hostname=" variable is optional (used to change the server name)

# if dcs_exists: false and dcs_type: "etcd" (in vars/main.yml)
[etcd_cluster]  # recommendation: 3 or 5-7 nodes
20.0.0.101
20.0.0.102
20.0.0.103

# if with_haproxy_load_balancing: true (in vars/main.yml)
[balancers]
20.0.0.101
20.0.0.102
20.0.0.103

# PostgreSQL nodes
[master]
20.0.0.101 hostname=pgnode01 postgresql_exists='false'

[replica]
20.0.0.102 hostname=pgnode02 postgresql_exists='false'
20.0.0.103 hostname=pgnode03 postgresql_exists='false'

[postgres_cluster:children]
master
replica

# In this example, all components will be installed on PostgreSQL nodes
# You can deploy the etcd cluster and the haproxy balancers on other dedicated servers. 

# if pgbackrest_install: true and "repo_host" is set (in vars/main.yml)
[pgbackrest]  # optional (Dedicated Repository Host)

# Connection settings
[all:vars]
ansible_connection='ssh'
ansible_ssh_port='22'
ansible_user='vagrant'
ansible_ssh_pass='vagrant'  # "sshpass" package is required for use "ansible_ssh_pass"
ansible_ssh_private_key_file=~/.ssh/id_rsa.pub
# ansible_python_interpreter='/usr/bin/python3'  # is required for use python3

[pgbackrest:vars]
ansible_user='vagrant'
ansible_ssh_pass='vagrant'

